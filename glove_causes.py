# -*- coding: utf-8 -*-
"""
Created on Sun Aug 27 22:46:06 2017

@author: Moji
"""

from keras.preprocessing.text import Tokenizer
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical
import numpy as np
import pandas as pd
import os

from keras.layers import Embedding
from keras.layers import Dense, Input, Flatten
from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional
from keras.models import Model



from collections import Counter
from keras.callbacks import ModelCheckpoint
#import madules that contain implementation and evaluation of deep learning part
from ModelsAndEvaluation import build_model,load_best_model,evaluation_plot,save_history_plot,cross_val


trains = pd.read_csv('C:\\Users\\Moji\\total_reports6.csv', encoding='ISO-8859-1')
trains['cause_letter']= trains['cause_letter'].astype(str)
Counter(trains['cause_letter'])
trains['COSTCAT']=trains['COSTCAT'].astype(int)
trains['COSTCAT'].describe()
causes_categories = pd.Categorical(trains['cause_letter'])
causes_categories = causes_categories.codes
print(causes_categories)
causes_categories.shape
labels = to_categorical((causes_categories))
labels.shape
content= ['I believe this is the most interesting task in my hands','I hate this task','TRAIN NO.#4 WITH ENGS 83/11/90/44 AND 11 CARS DERAILED 2 DEADHEAD CARS, C/44834 AND C/9639, WHILE MAKING A SHOVING MOVE ONTO TRACK 28.  THE DERAILMENT WAS DUE TO HIGH BUFF FORCES CAUSED JACKKNIFING OFDEADHEADING AMFLEET CAR 44834 LOCATED DIRECTLY BEHIND ENGINES DUE TO EXCESSIVE AMPERAGE GENERATED BY FOUR P42 LOCOMOTIVES SHOVING TRAIN AGAINST AN APPROXIMATELY 15-POUND BRAKE REDUCTION.']
content=trains['naritive'].astype(str)
tokenizer = Tokenizer()
tokenizer.fit_on_texts(content)

sequences = tokenizer.texts_to_sequences(content)
word_index = tokenizer.word_index


data = pad_sequences(sequences, maxlen=500)


indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]
nb_validation_samples = int(0.1 * data.shape[0])

x_train = data[:-nb_validation_samples]
y_train = labels[:-nb_validation_samples]
x_val = data[-nb_validation_samples:]
y_val = labels[-nb_validation_samples:]

GLOVE_DIR = "/Users/Moji/Downloads/"
embeddings_index = {}
f = open(os.path.join(GLOVE_DIR,'glove.6B.100d.txt'),encoding="utf8")
for line in f:
    values = line.split()
    word = values[0]
    #print(word)
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()



EMBEDDING_DIM = 100
embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector

n=5
####### CNN model from here ######## 
####################################
cnn_model =build_model("CNN",embedding_matrix,word_index,n, 'categorical_crossentropy')
model.summary()
#model_2 is going to be used to upload the loads for the best model during epoches
cnn_model_2=cnn_model
# checkpoint
filepath="cnn.glv.weights.best.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]
cnn_history = cnn_model.fit(x_train, y_train, validation_data=(x_val, y_val),
          nb_epoch=10, batch_size=128,callbacks=callbacks_list, verbose=1)
# using checkpoint list and filepath for weights load the best model during the training
load_best_model(cnn_model_2,filepath)
###################### evaluatins #############
evaluation_plot(cnn_model_2, x_val, y_val, ['E', 'H',  'M' ,'S' ,'T'], 'new_cnn_causes_glv.pdf')


################ plots########
save_history_plot(cnn_history, 'cnn_history_causes_acc_glv.pdf')

cross_val(data, labels,"CNN", embedding_matrix,word_index,n,'sparse_categorical_crossentropy', 10)


####### RNN model from here ######## 
####################################
n=5
rnn_model =build_model("RNN",embedding_matrix,word_index,n, 'categorical_crossentropy')
rnn_model.summary()
#model_2 is going to be used to upload the loads for the best model during epoches
rnn_model_2=rnn_model
# checkpoint
filepath="rnn.glv.weights.best.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]
rnn_history = rnn_model.fit(x_train, y_train, validation_data=(x_val, y_val),
          nb_epoch=8, batch_size=128,callbacks=callbacks_list, verbose=1)
# using checkpoint list and filepath for weights load the best model during the training
load_best_model(rnn_model_2,filepath)
###################### evaluatins #############
evaluation_plot(rnn_model_2, x_val, y_val, ['E', 'H',  'M' ,'S' ,'T'], 'new_rnn_causes_glv.pdf')


################ plots########
save_history_plot(rnn_history, 'rnn_history_causes_acc_glv.pdf')

cross_val(data, labels,"RNN", embedding_matrix,word_index,n,'sparse_categorical_crossentropy', 6)



